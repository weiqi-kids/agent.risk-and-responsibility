---
title: "What if AI Knew When to Say \"I Don't Know\"?"
source_url: https://cloudsecurityalliance.org/articles/what-if-ai-knew-when-to-say-i-don-t-know
date: 2026-01-21
category: ai_security
confidence: 高
---

## L1 — Rule Signal
- **rule_type**: guidance
- **issuing_body**: Cloud Security Alliance (CSA)
- **document_id**: —
- **status**: final

## L2 — Responsibility Structure
- **affected_roles**: AI 系統開發者、AI 治理團隊、風險管理團隊、研究人員
- **shift_type**: new
- **shift_summary**: AI 缺乏不確定性*意識*(非詞彙),必須從對沖語言轉向證據閘控輸出,並在模型架構中實現原生認識論追蹤

## L3 — Risk Domains
- AI Security
- Epistemic Uncertainty
- AI Governance
- Autonomous Systems
- Training Data Quality

## L4 — Obligation & Evidence
- **new_obligations**:
  - 要求來源和正當性,而非僅答案
  - 設定閾值:若無法找到足夠高品質來源,模型應明說
  - 明確分離事實聲明與推論
  - 詢問「此推理中最薄弱的環節是什麼?」
  - 透過要求反論測試提示
  - 從*對沖語言*轉向*證據閘控輸出*——更像情報技藝而非禮貌對話
  - 治理需知道不確定性*所在*——在資料、推論、範圍,或連結前提與結論的鏈路中
  - 需要更多審議層資料(爭議日誌、結構化認識論標籤)
- **evidence_requirements**:
  - 來源引用與正當性追蹤
  - 事實聲明與推論的明確分離
  - 推理鏈弱點分析
  - 反論測試記錄
  - 證據閘控輸出機制
- **enforcement_signal**: recommended

## 核心問題
AI 不缺乏不確定性*詞彙*——它缺乏不確定性*意識*。模型可以按需說「我不確定」,但這是**表現,而非感知**。更深層問題:「無充分正當性的斷言」。

Token 層級信心(預測下一個詞)根本上不同於聲明層級信心(知道一個*想法*是否有根據)。語法流暢的段落仍可能完全錯誤。

## 當前方法為何失敗
**提示推理鏈**是反應性的,而非內在的——模型產生正當性*因為被問*,而非因為它們原生追蹤認識論狀態。

**事後驗證層**在自信聽起來的錯誤已發出後,將系統置於「清理模式」。更糟的是,若監督層缺乏更好的認識論基礎,問題只是向上遷移。

**事實不確定性**相對易處理(可檢查聲明、可驗證來源)。**推理鏈不確定性**真正更難——每個推論步驟可能個別可辯護,但整體鏈路默默崩潰。

## Wikipedia 訊號缺口
Wikipedia 提供罕見的訓練資源:明確認識論元資料(`[citation needed]`、`[disputed]`、`[original research]`)加上討論頁中可見的審議,人們挑戰*事實和推論*。大多數訓練資料是已編輯掉審議的精緻最終輸出。文章識別出「稀缺階梯」:
- **最常見**:最終形式輸出
- **較罕見**:修訂歷史
- **最罕見/最有價值**:「明確認識論爭議」——關於*為何*聲明成立的可見論證

## 提議解決方案
**近期實用步驟**:
- 要求來源和正當性,而非僅答案
- 設定閾值:若無法找到足夠高品質來源,模型應明說
- 明確分離事實聲明與推論
- 詢問「此推理中最薄弱的環節是什麼?」
- 透過要求反論測試提示

**更深層目標**:從*對沖語言*轉向*證據閘控輸出*——更像情報技藝而非禮貌對話。例如:「此結論依賴假設 X,我無法驗證。」

## AI 設計與治理影響
| 層級 | 影響 |
|------|------|
| **模型架構** | 原生認識論追蹤優於附加驗證器 |
| **Agentic 系統** | 當 AI 自主委派任務時,風險升級——無正當性的信心跨代理複合 |
| **訓練資料** | 需要更多審議層資料(爭議日誌、結構化認識論標籤) |
| **治理** | 風險管理需知道不確定性*所在*——在資料、推論、範圍,或連結前提與結論的鏈路中 |

## Notes
開放研究問題:模型能否*學習*內在追蹤正當性狀態,或推理鏈不確定性在結構上比事實不確定性更難內化?文章將此留為真正未解決。核心論點:這不是詞彙問題,而是認知架構問題。
