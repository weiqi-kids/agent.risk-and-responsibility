---
title: "Protecting Trained Models in Privacy-Preserving Federated Learning"
source_url: https://www.nist.gov/blogs/cybersecurity-insights/protecting-trained-models-privacy-preserving-federated-learning
date: 2024-07-15
category: privacy
confidence: 中
---

## L1 — Rule Signal
- **rule_type**: guidance
- **issuing_body**: NIST (in collaboration with UK Responsible Technology Adoption Unit)
- **document_id**: N/A (blog series post)
- **status**: final

## L2 — Responsibility Structure
- **affected_roles**: privacy-preserving federated learning practitioners, model security specialists, output privacy implementers
- **shift_type**: clarified
- **shift_summary**: 說明如何保護隱私保護聯合學習中的訓練模型（trained models），涵蓋 input privacy（前兩篇已討論 horizontally 與 vertically partitioned data）與 output privacy 技術

## L3 — Risk Domains
- Privacy-preserving federated learning
- Trained model protection
- Output privacy
- Model inference attacks

## L4 — Obligation & Evidence
- **new_obligations**: N/A (technical guidance)
- **evidence_requirements**: N/A
- **enforcement_signal**: informational

## Notes
本文為 NIST-UK RTA 合作系列之一，前兩篇已討論 input privacy for horizontally 與 vertically partitioned data，本篇聚焦於保護訓練模型（protecting trained models）以完整涵蓋 PPFL 隱私保護。系列文章發布於 NIST Privacy Engineering Collaboration Space 與 RTA blog。Description < 150 字但文章性質明確（系列第三篇），未使用 WebFetch。
